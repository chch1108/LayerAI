import streamlit as st
import os
import tempfile
import io
import zipfile
import pandas as pd
import numpy as np
from PIL import Image

# --- è‡ªè¨‚æ¨¡çµ„ ---
from image_processor import (
    extract_images_from_zip,
    batch_predict_layers,
    make_plotly_heatmap_and_curve,
    estimate_time_and_effects
)
from image_editor_level1 import overlay_issue_markers
from model_train import load_model_and_predict, INPUT_FEATURES
from llm_recommender import llm_highrisk_feedback

# --- Streamlit è¨­å®š ---
st.set_page_config(layout="wide", page_title="LayerAI - Multi-layer Suite")
st.title("LayerAI â€” å¤šå±¤é€å±¤é æ¸¬ã€Auto-Tuneã€ä¿®æ­£ç‰ˆåˆ‡ç‰‡èˆ‡æ•ˆç›Šå„€è¡¨æ¿")

st.markdown("""
ä¸Šå‚³åŒ…å«åˆ‡ç‰‡çš„ ZIPï¼ˆæ¯å±¤ png/jpgï¼‰ã€‚  
ç³»çµ±æœƒä¾åºå®Œæˆï¼š

1. é€å±¤å›æµé¢¨éšªé æ¸¬  
2. Heatmap + é¢¨éšªæ›²ç·š  
3. Level 1 ä¿®æ­£ç‰ˆåˆ‡ç‰‡ï¼ˆç•«æ¡†æ¨™è¨˜é¢¨éšªï¼‰  
4. é«˜é¢¨éšªå±¤ LLM å»ºè­° / çµè«–  
5. æˆæ•ˆå„€è¡¨æ¿ï¼šæ™‚é–“ç¯€çœèˆ‡æˆåŠŸç‡æå‡
""")

col1, col2 = st.columns([1, 2])
with col1:
    uploaded = st.file_uploader("ä¸Šå‚³åˆ‡ç‰‡ ZIP æª” (æ¯å¼µç‚ºä¸€å±¤)", type=["zip"])
    threshold = st.slider("é«˜é¢¨éšªåˆ¤å®šé–¾å€¼ï¼ˆfailure probabilityï¼‰",
                          min_value=0.0, max_value=1.0,
                          value=0.5, step=0.01)
    run_btn = st.button("é–‹å§‹åˆ†æï¼ˆå…¨æµç¨‹ï¼‰")

if uploaded and run_btn:
    with tempfile.TemporaryDirectory() as tmpdir:
        zip_path = os.path.join(tmpdir, "slices.zip")
        with open(zip_path, "wb") as f:
            f.write(uploaded.getbuffer())

        st.info("è§£å£“ä¸¦è®€å–åˆ‡ç‰‡...")
        imgs, filenames = extract_images_from_zip(zip_path, tmpdir)
        st.success(f"è®€å– {len(imgs)} å¼µåˆ‡ç‰‡")

        # ---------------------------------------------
        # Step 1ï¼šé€å±¤é æ¸¬
        # ---------------------------------------------
        st.info("é€å±¤é€²è¡Œæ¨¡å‹é æ¸¬...")

        results = []
        high_risk_count = 0
        total_prob_list = []

        for idx, (img, fname) in enumerate(zip(imgs, filenames)):
            # å‡è¨­ extract_features_from_image(img) æœƒå›å‚³æ¯å±¤çš„ç‰¹å¾µå­—å…¸
            geo_features = {
                "area": img.size[0] * img.size[1],  # ç¯„ä¾‹
                "perimeter": 2*(img.size[0]+img.size[1]),
                "hydraulic_diameter": np.sqrt(4*(img.size[0]*img.size[1])/(2*(img.size[0]+img.size[1])))
            }

            # å‡è¨­å›ºå®šç¯„ä¾‹åˆ—å°åƒæ•¸ï¼ˆå¯¦éš›å¯æ”¹æˆå¾æª”åæˆ– metadata è®€å–ï¼‰
            input_data = {
                'ææ–™é»åº¦ (cps)': 500,
                'æŠ¬å‡é«˜åº¦(Î¼m)': 6.0,
                'æŠ¬å‡é€Ÿåº¦(Î¼m/s)': 2.0,
                'ç­‰å¾…æ™‚é–“(s)': 4.5,
                'ä¸‹é™é€Ÿåº¦((Î¼m)/s)': 5.0,
                'å½¢ç‹€': 'æ–¹å½¢',
                'é¢ç©(mm?)': geo_features['area'],
                'å‘¨é•·(mm)': geo_features['perimeter'],
                'æ°´åŠ›ç›´å¾‘(mm)': geo_features['hydraulic_diameter']
            }
            final_input_data = {feat: input_data.get(feat) for feat in INPUT_FEATURES}
            input_df = pd.DataFrame([final_input_data])

            try:
                prediction, importances = load_model_and_predict(input_df)
                prob = float(prediction)  # 0 æˆ– 1ï¼Œç¤ºæ„
                total_prob_list.append(prob)

                if prob >= threshold:
                    high_risk_count += 1

                results.append({
                    "layer": idx+1,
                    "filename": fname,
                    "prob": prob
                })

            except Exception as e:
                st.warning(f"ç¬¬ {idx+1} å±¤é æ¸¬å¤±æ•—: {e}")
                results.append({
                    "layer": idx+1,
                    "filename": fname,
                    "prob": 0.0
                })
                total_prob_list.append(0.0)

        results_df = pd.DataFrame(results)
        st.dataframe(results_df.head(50))

        # ---------------------------------------------
        # Step 2ï¼šHeatmap & æ›²ç·š
        # ---------------------------------------------
        st.info("ç”Ÿæˆ heatmap èˆ‡é¢¨éšªæ›²ç·š...")
        risks = results_df["prob"].values
        heatmap_fig, curve_fig = make_plotly_heatmap_and_curve(risks)
        st.plotly_chart(heatmap_fig, use_container_width=True)
        st.plotly_chart(curve_fig, use_container_width=True)

        # ---------------------------------------------
        # Step 3ï¼šLevel 1 ä¿®æ­£ï¼ˆOverlayï¼‰
        # ---------------------------------------------
        st.info("ç”Ÿæˆ Level 1 ä¿®æ­£ç‰ˆåˆ‡ç‰‡ï¼ˆç•«æ¡†ç‰ˆï¼‰...")

        modified_images = []
        modified_filenames = []

        for img, fname, prob in zip(imgs, filenames, risks):
            mod_img = overlay_issue_markers(img, prob)
            modified_images.append(mod_img)
            modified_filenames.append(fname)

        st.subheader("ä¿®æ­£å¾Œçš„åˆ‡ç‰‡ï¼ˆLevel 1 Overlayï¼‰")
        for fname, mod_img, prob in zip(modified_filenames, modified_images, risks):
            st.image(mod_img, caption=f"{fname} â€” é¢¨éšª {prob:.2f}", use_column_width=True)

        # ---------------- ZIP æ‰“åŒ… -------------------
        st.info("å£“ç¸®ä¿®æ­£ç‰ˆåˆ‡ç‰‡ ZIP...")
        zip_buf = io.BytesIO()
        with zipfile.ZipFile(zip_buf, "w") as z:
            for fname, img in zip(modified_filenames, modified_images):
                img_bytes = io.BytesIO()
                img.save(img_bytes, format="PNG")
                z.writestr(fname, img_bytes.getvalue())

        st.download_button(
            "â¬‡ï¸ ä¸‹è¼‰ä¿®æ­£ç‰ˆåˆ‡ç‰‡ ZIP",
            data=zip_buf.getvalue(),
            file_name="modified_slices.zip",
            mime="application/zip",
        )

        # ---------------------------------------------
        # Step 4ï¼šLLM é«˜é¢¨éšªå±¤å»ºè­° / çµè«–
        # ---------------------------------------------
        st.info("ç”¢ç”Ÿ LLM é«˜é¢¨éšªå±¤å»ºè­° / çµè«–...")

        stats_summary = {
            "total_layers": len(results_df),
            "high_risk_layers": high_risk_count,
            "avg_prob": np.mean(total_prob_list) if total_prob_list else 0,
            "max_prob": np.max(total_prob_list) if total_prob_list else 0
        }

        with st.spinner("LLM æ­£åœ¨ç”Ÿæˆå»ºè­°ï¼Œè«‹ç¨å€™..."):
            llm_text = llm_highrisk_feedback(stats_summary, threshold=threshold)

        st.subheader("ğŸ¤– AI é«˜é¢¨éšªå±¤å»ºè­° / çµè«–")
        st.markdown(llm_text)

        # ---------------------------------------------
        # Step 5ï¼šæ•ˆç›Šå„€è¡¨æ¿
        # ---------------------------------------------
        st.info("è¨ˆç®—æ™‚é–“ç¯€çœèˆ‡æˆåŠŸç‡æ”¹å–„é ä¼°...")
        time_report_df = estimate_time_and_effects(results_df)

        st.subheader("æ™‚é–“èˆ‡æˆåŠŸç‡æ”¹å–„é ä¼°")
        st.dataframe(time_report_df)

        st.download_button(
            "ä¸‹è¼‰æ™‚é–“æ•ˆç›Šå ±å‘Š CSV",
            data=time_report_df.to_csv(index=False).encode('utf-8'),
            file_name="time_effects_report.csv",
            mime="text/csv"
        )

        st.success("åˆ†æå®Œæˆï¼")
